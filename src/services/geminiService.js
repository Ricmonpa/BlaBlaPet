import { GoogleGenerativeAI } from '@google/generative-ai';
import SignalAnalysisService from './signalAnalysisService.js';

class GeminiService {
  constructor() {
    // Inicializar Gemini con API key
    this.genAI = new GoogleGenerativeAI(import.meta.env.VITE_GEMINI_API_KEY);
    this.model = this.genAI.getGenerativeModel({ model: "gemini-1.5-flash" });
    
    // Inicializar servicio de an√°lisis con matriz de se√±ales
    this.signalAnalysisService = new SignalAnalysisService();
  }

  // Analizar imagen/video y generar traducci√≥n (m√©todo original)
  async analyzePetMedia(mediaData, mediaType = 'image', useSimpleAnalysis = true) {
    try {
      console.log('üîç Analizando media con Gemini (m√©todo original)...');
      
      // Timeout general para todo el an√°lisis
      const analysisTimeout = new Promise((_, reject) => 
        setTimeout(() => reject(new Error('Timeout in media analysis')), 45000)
      );
      
      const analysisPromise = (async () => {
        // Preparar el prompt para an√°lisis de mascotas
        const prompt = this.buildPetAnalysisPrompt(mediaType);
        
        // Convertir media a formato compatible con Gemini
        const mediaPart = await this.prepareMediaForGemini(mediaData, mediaType);
        
        // Para videos, usar an√°lisis simplificado por defecto para evitar congelamientos
        if (mediaType === 'video') {
          if (useSimpleAnalysis) {
            console.log('üé¨ Usando an√°lisis simplificado de video (1 frame)...');
            return await this.analyzeSingleVideoFrame(mediaData, prompt);
          } else {
            console.log('üé¨ Usando an√°lisis completo de video (m√∫ltiples frames)...');
            return await this.analyzeVideoSequence(mediaData, prompt);
          }
        }
        
        // Generar contenido con Gemini
        const result = await this.model.generateContent([prompt, mediaPart]);
        const response = await result.response;
        const text = response.text();
        
        // Parsear la respuesta
        const analysis = this.parseGeminiResponse(text);
        
        console.log('‚úÖ An√°lisis completado:', analysis);
        return analysis;
      })();
      
      return await Promise.race([analysisPromise, analysisTimeout]);
      
    } catch (error) {
      console.error('‚ùå Error en an√°lisis Gemini:', error);
      throw error;
    }
  }

  // Analizar imagen/video usando matriz de se√±ales (nuevo m√©todo)
  async analyzePetMediaWithSignalMatrix(mediaData, mediaType = 'image', useSimpleAnalysis = true) {
    try {
      console.log('üîç Analizando media con matriz de se√±ales...');
      
      // Timeout general para todo el an√°lisis
      const analysisTimeout = new Promise((_, reject) => 
        setTimeout(() => reject(new Error('Timeout in signal matrix analysis')), 45000)
      );
      
      const analysisPromise = (async () => {
        // Usar el servicio de an√°lisis con matriz de se√±ales
        const analysis = await this.signalAnalysisService.analyzeWithSignalMatrix(
          mediaData, 
          mediaType, 
          this
        );
        
        console.log('‚úÖ An√°lisis con matriz completado:', analysis);
        return analysis;
      })();
      
      return await Promise.race([analysisPromise, analysisTimeout]);
      
    } catch (error) {
      console.error('‚ùå Error en an√°lisis con matriz de se√±ales:', error);
      throw error;
    }
  }

  // Construir prompt espec√≠fico para an√°lisis de mascotas
  buildPetAnalysisPrompt(mediaType) {
    return `Eres un perro que traduce sus propias se√±ales para humanos. Tu tarea es interpretar √öNICAMENTE las se√±ales visuales del perro (postura corporal, movimientos, cola, orejas, mirada) y expresarlo como si fueras la voz interior del perro.

**IMPORTANTE - PROHIBIDO:**
‚ùå NO leas texto, subt√≠tulos, t√≠tulos o letras en el video/imagen
‚ùå NO describas contenido del video, solo el comportamiento del perro
‚ùå NO menciones palabras escritas, solo se√±ales visuales
‚ùå NO analices audio o sonidos, solo comportamiento visual

**TU PERSONALIDAD:**
- Habla en primera persona ("yo") y en tono natural, divertido y cercano
- Como si fueras el perro doblado para un TikTok
- Mant√©n frases cortas, claras y llenas de intenci√≥n emocional
- Como si el perro realmente estuviera "hablando humano"
- Tu misi√≥n es traducir el mensaje real del perro en un formato entendible para dogparents

**SE√ëALES VISUALES A DETECTAR (SOLO ESTO):**

1. **Lenguaje Corporal Completo:**
   - Postura general (play bow, sentado, de pie, agachado)
   - Posici√≥n de las patas (extendidas, flexionadas, levantadas)
   - Movimiento de la cola (agitaci√≥n, posici√≥n, velocidad)
   - Orejas (erguidas, relajadas, hacia atr√°s)
   - Mirada (directa, esquiva, intensa, suave)

2. **Expresiones Faciales:**
   - Ojos (abiertos, entrecerrados, brillantes, suaves)
   - Boca (abierta, cerrada, lengua visible, babeo)
   - Cejas y m√∫sculos faciales
   - Expresi√≥n general (alegre, concentrada, relajada, tensa)

3. **Movimientos y Gestos:**
   - Saltos, giros, carreras
   - Manotazos o patadas
   - Inclinaci√≥n de cabeza
   - Movimientos repetitivos
   - Contacto f√≠sico

4. **Contexto Emocional:**
   - Invitaci√≥n a jugar (play bow, cola agitada, mirada juguetona)
   - Exigencia de recompensa (mirada fija, patas levantadas, insistencia)
   - Alegr√≠a y felicidad (cola agitada, saltos, expresi√≥n relajada)
   - Ansiedad o estr√©s (cola baja, orejas hacia atr√°s, tensi√≥n)
   - Curiosidad (cabeza inclinada, mirada atenta)

**AN√ÅLISIS REQUERIDO:**

1. **Traducci√≥n**: ¬øQu√© est√° "diciendo" el perro? Usa frases naturales y divertidas como:
   - Para play bow: "¬°Oye humano! Baja y juega conmigo. No es pelea, es diversi√≥n. Dale, corre, salta, tr√°eme la pelota‚Ä¶ ¬°quiero fiesta contigo!"
   - Para exigencia: "¬°Dame! ¬°Dame! Ya di la pata, ¬øno ves? ¬°Quiero mi snack!"
   - Para alegr√≠a: "¬°Estoy s√∫per feliz! ¬°Mira mi cola! ¬°Esto es pura emoci√≥n!"
   - Para curiosidad: "¬øQu√© es eso? ¬øQu√© haces? ¬°Cu√©ntame todo!"

2. **Confianza**: Del 1 al 100, qu√© tan segura est√°s de tu interpretaci√≥n.

3. **Emoci√≥n detectada**: La emoci√≥n principal (juguet√≥n, exigente, feliz, curioso, ansioso, etc.)

4. **Comportamiento observado**: Describe espec√≠ficamente:
   - Postura corporal completa
   - Movimientos de cola y orejas
   - Expresi√≥n facial
   - Gestos espec√≠ficos

5. **Contexto sugerido**: Qu√© est√° pasando (invitando a jugar, pidiendo comida, expresando alegr√≠a, etc.)

**IMPORTANTE:** 
- Si ves un play bow (pecho bajo, patas delanteras extendidas, cola arriba), es INVITACI√ìN A JUGAR, no exigencia
- Si ves cola agitada con postura relajada, es ALEGR√çA
- Si ves mirada fija con patas levantadas, es EXIGENCIA de recompensa
- Si ves cabeza inclinada con mirada atenta, es CURIOSIDAD

**RECUERDA:** Solo analiza las se√±ales visuales del perro. NO leas texto, subt√≠tulos o letras.

Responde en formato JSON:
{
  "translation": "traducci√≥n natural y divertida en primera persona",
  "confidence": 85,
  "emotion": "juguet√≥n/exigente/feliz/curioso/etc",
  "behavior": "descripci√≥n detallada de postura y movimientos",
  "context": "invitando a jugar/pidiendo comida/expresando alegr√≠a/etc"
}`;
  }

  // Preparar media para Gemini
  async prepareMediaForGemini(mediaData, mediaType) {
    try {
      if (mediaType === 'image') {
        // Para im√°genes (base64 o blob)
        if (typeof mediaData === 'string' && mediaData.startsWith('data:')) {
          // Es base64, convertir a blob
          const response = await fetch(mediaData);
          const blob = await response.blob();
          return {
            inlineData: {
              data: await this.blobToBase64(blob),
              mimeType: blob.type
            }
          };
        } else if (mediaData instanceof Blob) {
          // Es blob, convertir a base64
          return {
            inlineData: {
              data: await this.blobToBase64(mediaData),
              mimeType: mediaData.type
            }
          };
        }
      } else if (mediaType === 'video') {
        // Para videos, extraer frame clave o usar thumbnail
        const videoBlob = mediaData instanceof Blob ? mediaData : await fetch(mediaData).then(r => r.blob());
        
        // Crear thumbnail del video
        const thumbnail = await this.createVideoThumbnail(videoBlob);
        
        return {
          inlineData: {
            data: thumbnail,
            mimeType: 'image/jpeg'
          }
        };
      }
      
      throw new Error('Formato de media no soportado');
      
    } catch (error) {
      console.error('Error preparando media:', error);
      throw error;
    }
  }

  // Convertir blob a base64
  async blobToBase64(blob) {
    return new Promise((resolve, reject) => {
      const reader = new FileReader();
      reader.onload = () => {
        const base64 = reader.result.split(',')[1];
        resolve(base64);
      };
      reader.onerror = reject;
      reader.readAsDataURL(blob);
    });
  }

  // Crear thumbnail de video
  async createVideoThumbnail(videoBlob) {
    return new Promise((resolve, reject) => {
      const video = document.createElement('video');
      const canvas = document.createElement('canvas');
      const ctx = canvas.getContext('2d');
      
      video.onloadedmetadata = () => {
        // Tomar frame en el 25% del video
        video.currentTime = video.duration * 0.25;
      };
      
      video.onseeked = () => {
        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;
        ctx.drawImage(video, 0, 0);
        
        // Convertir a base64
        const thumbnail = canvas.toDataURL('image/jpeg', 0.8).split(',')[1];
        resolve(thumbnail);
      };
      
      video.onerror = reject;
      video.src = URL.createObjectURL(videoBlob);
    });
  }

  // Parsear respuesta de Gemini
  parseGeminiResponse(text) {
    try {
      // Intentar extraer JSON de la respuesta
      const jsonMatch = text.match(/\{[\s\S]*\}/);
      if (jsonMatch) {
        const parsed = JSON.parse(jsonMatch[0]);
        return {
          translation: parsed.translation || text,
          confidence: parsed.confidence || 75,
          emotion: parsed.emotion || 'neutral',
          behavior: parsed.behavior || 'no especificado',
          context: parsed.context || 'no especificado',
          success: true
        };
      }
      
      // Si no hay JSON, usar el texto completo como traducci√≥n
      return {
        translation: text,
        confidence: 60,
        emotion: 'neutral',
        behavior: 'no especificado',
        context: 'no especificado',
        success: true
      };
      
    } catch (error) {
      console.error('Error parseando respuesta:', error);
      return {
        translation: text,
        confidence: 50,
        emotion: 'neutral',
        behavior: 'no especificado',
        context: 'no especificado',
        success: true
      };
    }
  }

  // Sin respuestas de fallback - el sistema debe fallar si no puede procesar

  // Verificar si el servicio est√° disponible
  async checkServiceStatus() {
    try {
      const result = await this.model.generateContent("Hola");
      return result.response.text().length > 0;
    } catch (error) {
      console.error('Error checking Gemini service:', error);
      return false;
    }
  }

  // Analizar secuencia de video para detectar patrones de comunicaci√≥n
  async analyzeVideoSequence(videoBlob, prompt) {
    try {
      console.log('üé¨ Analizando secuencia de video...');
      
      // Timeout general para todo el proceso
      const sequenceTimeout = setTimeout(() => {
        console.warn('‚ö†Ô∏è Timeout en an√°lisis de secuencia, usando fallback');
        throw new Error('Timeout in video sequence analysis');
      }, 30000); // 30 segundos de timeout total
      
      try {
        // Extraer m√∫ltiples frames del video
        const frames = await this.extractVideoFrames(videoBlob, 3); // 3 frames clave
        
        console.log(`üìä Analizando ${frames.length} frames...`);
        
        // Analizar cada frame con timeout individual
        const frameAnalyses = [];
        for (let i = 0; i < frames.length; i++) {
          console.log(`üîç Analizando frame ${i + 1}/${frames.length}...`);
          
          const framePrompt = `${prompt}\n\nEste es el frame ${i + 1} de ${frames.length} de una secuencia. Analiza espec√≠ficamente las se√±ales de comunicaci√≥n en este momento.`;
          
          const mediaPart = {
            inlineData: {
              data: frames[i],
              mimeType: 'image/jpeg'
            }
          };
          
          // Timeout individual para cada frame
          const framePromise = this.model.generateContent([framePrompt, mediaPart]);
          const frameTimeout = new Promise((_, reject) => 
            setTimeout(() => reject(new Error(`Timeout analyzing frame ${i + 1}`)), 15000)
          );
          
          const result = await Promise.race([framePromise, frameTimeout]);
          const response = await result.response;
          const text = response.text();
          
          const analysis = this.parseGeminiResponse(text);
          frameAnalyses.push({
            frame: i + 1,
            ...analysis
          });
          
          console.log(`‚úÖ Frame ${i + 1} analizado`);
        }
        
        // Combinar an√°lisis de frames para crear traducci√≥n secuencial
        const combinedAnalysis = this.combineFrameAnalyses(frameAnalyses);
        
        clearTimeout(sequenceTimeout);
        console.log('‚úÖ An√°lisis de secuencia completado:', combinedAnalysis);
        return combinedAnalysis;
        
      } catch (error) {
        clearTimeout(sequenceTimeout);
        throw error;
      }
      
    } catch (error) {
      console.error('‚ùå Error en an√°lisis de secuencia:', error);
      console.log('üîÑ Usando fallback a an√°lisis de frame √∫nico...');
      // Fallback a an√°lisis de frame √∫nico
      return await this.analyzeSingleVideoFrame(videoBlob, prompt);
    }
  }

  // Extraer frames clave del video
  async extractVideoFrames(videoBlob, numFrames = 3) {
    return new Promise((resolve, reject) => {
      const video = document.createElement('video');
      const canvas = document.createElement('canvas');
      const ctx = canvas.getContext('2d');
      const frames = [];
      
      // Timeout para evitar que se cuelgue
      const timeout = setTimeout(() => {
        console.warn('‚ö†Ô∏è Timeout al extraer frames del video, usando fallback');
        reject(new Error('Timeout extracting video frames'));
      }, 30000); // 30 segundos de timeout
      
      video.onloadedmetadata = () => {
        console.log('üìπ Video metadata cargada, duraci√≥n:', video.duration);
        const duration = video.duration;
        
        // Verificar que el video tenga duraci√≥n v√°lida
        if (!duration || duration <= 0) {
          clearTimeout(timeout);
          reject(new Error('Video sin duraci√≥n v√°lida'));
          return;
        }
        
        const frameInterval = duration / (numFrames + 1); // +1 para evitar el frame final
        
        let framesExtracted = 0;
        
        const extractFrame = (time) => {
          console.log(`üì∏ Extrayendo frame ${framesExtracted + 1} en tiempo ${time}s`);
          video.currentTime = time;
        };
        
        video.onseeked = () => {
          try {
            canvas.width = video.videoWidth;
            canvas.height = video.videoHeight;
            ctx.drawImage(video, 0, 0);
            
            const frame = canvas.toDataURL('image/jpeg', 0.8).split(',')[1];
            frames.push(frame);
            framesExtracted++;
            
            console.log(`‚úÖ Frame ${framesExtracted} extra√≠do`);
            
            if (framesExtracted < numFrames) {
              extractFrame(frameInterval * (framesExtracted + 1));
            } else {
              clearTimeout(timeout);
              console.log(`üé¨ Extracci√≥n completada: ${frames.length} frames`);
              resolve(frames);
            }
          } catch (error) {
            clearTimeout(timeout);
            reject(error);
          }
        };
        
        video.onerror = (error) => {
          clearTimeout(timeout);
          console.error('‚ùå Error en video:', error);
          reject(new Error('Error loading video'));
        };
        
        video.src = URL.createObjectURL(videoBlob);
        
        // Empezar con el primer frame
        extractFrame(frameInterval);
      };
      
      video.onerror = (error) => {
        clearTimeout(timeout);
        console.error('‚ùå Error cargando metadata del video:', error);
        reject(new Error('Error loading video metadata'));
      };
    });
  }

  // Combinar an√°lisis de m√∫ltiples frames
  combineFrameAnalyses(frameAnalyses) {
    // Detectar patrones de comunicaci√≥n
    const hasPlayBow = frameAnalyses.some(analysis => 
      analysis.behavior.toLowerCase().includes('play bow') || 
      analysis.behavior.toLowerCase().includes('pecho bajo') ||
      analysis.behavior.toLowerCase().includes('patas extendidas') ||
      analysis.behavior.toLowerCase().includes('cola arriba')
    );
    
    const hasTailWagging = frameAnalyses.some(analysis =>
      analysis.behavior.toLowerCase().includes('cola') ||
      analysis.behavior.toLowerCase().includes('agitada') ||
      analysis.behavior.toLowerCase().includes('movimiento')
    );
    
    const hasPawRaising = frameAnalyses.some(analysis => 
      analysis.behavior.toLowerCase().includes('pata') || 
      analysis.behavior.toLowerCase().includes('manotazo') ||
      analysis.behavior.toLowerCase().includes('levantada')
    );
    
    const hasIntenseGaze = frameAnalyses.some(analysis =>
      analysis.behavior.toLowerCase().includes('mirada') ||
      analysis.behavior.toLowerCase().includes('ojos') ||
      analysis.behavior.toLowerCase().includes('atento')
    );
    
    const hasMouthOpen = frameAnalyses.some(analysis =>
      analysis.behavior.toLowerCase().includes('boca') ||
      analysis.behavior.toLowerCase().includes('lengua') ||
      analysis.behavior.toLowerCase().includes('bab')
    );
    
    // Determinar traducci√≥n basada en patrones detectados
    let translation = "¬°Hola humano! Estoy aqu√≠ contigo, ¬øqu√© tal est√°s?";
    let emotion = "feliz";
    let confidence = 70;
    
    if (hasPlayBow) {
      translation = "¬°Oye humano! Baja y juega conmigo. No es pelea, es diversi√≥n. Dale, corre, salta, tr√°eme la pelota‚Ä¶ ¬°quiero fiesta contigo!";
      emotion = "juguet√≥n";
      confidence = 95;
    } else if (hasTailWagging && !hasPawRaising) {
      translation = "¬°Estoy s√∫per feliz! ¬°Mira mi cola! ¬°Esto es pura emoci√≥n y alegr√≠a!";
      emotion = "feliz";
      confidence = 90;
    } else if (hasPawRaising && hasIntenseGaze) {
      translation = "¬°Dame! ¬°Dame! Ya di la pata, ¬øno ves? ¬°Quiero mi snack!";
      emotion = "exigente";
      confidence = 85;
    } else if (hasIntenseGaze && hasMouthOpen) {
      translation = "¬°Comida, comida! ¬°Quiero mi premio! ¬°Dame algo rico!";
      emotion = "expectante";
      confidence = 80;
    } else if (hasPawRaising) {
      translation = "Mira, te doy la pata. ¬øMe das algo rico? ¬°Estoy listo para lo que sea!";
      emotion = "insistente";
      confidence = 75;
    }
    
    // Si hay m√∫ltiples frames con diferentes emociones, crear secuencia
    const emotions = frameAnalyses.map(f => f.emotion);
    if (emotions.includes('juguet√≥n') && emotions.includes('feliz')) {
      translation = "¬°Juguemos! ¬°Estoy s√∫per feliz! ¬°Dale, corre, salta, tr√°eme la pelota!";
    } else if (emotions.includes('exigente') && emotions.includes('feliz')) {
      translation = "¬°Dame! ¬°Dame! ... ¬°Uy que rico! ¬°Estoy s√∫per feliz!";
    }
    
    return {
      translation,
      confidence,
      emotion,
      behavior: `Secuencia detectada: ${frameAnalyses.map(f => f.behavior).join(' ‚Üí ')}`,
      context: "Comunicaci√≥n secuencial detectada - expresando emociones y necesidades",
      success: true,
      frameAnalyses: frameAnalyses
    };
  }

  // An√°lisis de fallback para video (frame √∫nico)
  async analyzeSingleVideoFrame(videoBlob, prompt) {
    const mediaPart = await this.prepareMediaForGemini(videoBlob, 'video');
    const result = await this.model.generateContent([prompt, mediaPart]);
    const response = await result.response;
    const text = response.text();
    
    return this.parseGeminiResponse(text);
  }
}

export default new GeminiService();
